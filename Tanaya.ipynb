{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124aea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates caption/query GloVe-200 embeddings\n",
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from cogworks_data.language import get_data_path\n",
    "import re, string\n",
    "from operator import add\n",
    "\n",
    "path = get_data_path(\"glove.6B.200d.txt.w2v\")\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5e9ec65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def embed_text(text):\n",
    "    text = strip_punc(text).lower()\n",
    "    text = text.split()\n",
    "    print(text)\n",
    "    \n",
    "    embedded = np.zeros(200)\n",
    "    for word in text:\n",
    "        if word in glove:\n",
    "            embedded += glove[word]\n",
    "            \n",
    "    return embedded\n",
    "\n",
    "punc_regex = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "def strip_punc(corpus):\n",
    "    return punc_regex.sub('', corpus)\n",
    "\n",
    "\n",
    "print(embed_text(\"Hello, world!\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3368d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# myNN model for embedding image descriptors\n",
    "import mygrad as mg\n",
    "import mynn\n",
    "from mynn.layers.dense import dense\n",
    "from mygrad.nnet.losses.margin_ranking_loss import margin_ranking_loss\n",
    "from mynn.optimizers.sgd import SGD\n",
    "from mygrad.nnet.initializers import glorot_normal\n",
    "from mygrad import no_autodiff\n",
    "import pickle\n",
    "from cogworks_data.language import get_data_path\n",
    "from pathlib import Path\n",
    "from Boueny import Database\n",
    "import Boueny\n",
    "\n",
    "\"\"\"\n",
    "(200, 512, 512) \n",
    "(200, 200, 200) (encoded)\n",
    "\n",
    "\n",
    "cosine similarity between actual image and caption \n",
    "cosine similarity between confuser image and caption\n",
    "\n",
    "loss function -> makes sure that actual image cosine similarity is always greater than confuser image\n",
    "\"\"\"\n",
    "def loss_function_accuracy(wtrueimg, wtruecap, wconfimg, margin): \n",
    "    \n",
    "    simtrue = mg.einsum(\"ni,ni -> n\", wtruecap, wtrueimg)\n",
    "\n",
    "    simconfuser = mg.einsum(\"ni,ni -> n\", wtruecap, wconfimg)\n",
    "\n",
    "    #returns both loss and accuracy, loss first\n",
    "    \n",
    "    accuracy = np.mean(simtrue.data  > simconfuser.data)  \n",
    "    \n",
    "    return (mg.mean(mg.maximum(0, margin - (simtrue - simconfuser))), accuracy)\n",
    "\n",
    "\n",
    "class Autoencoder:\n",
    "    def __init__(self, D_full, D_hidden):\n",
    "        self.encode = dense(D_full, D_hidden, weight_initializer=glorot_normal, bias=False)\n",
    "    \n",
    "    \n",
    "    def __call__(self, x):        \n",
    "        self.encode(x)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return self.encode.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b83c2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading GloVe\n",
      "loading coco dataset\n",
      "loading resnet18\n",
      "Loading image url data\n",
      "Loading captions\n",
      "0.0 0.5035981555734426 1.1840233863435923 2.2690977555405216\n",
      "generating caption embeddings\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Database' object has no attribute 'get_image_embeddings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m database \u001b[38;5;241m=\u001b[39m \u001b[43mDatabase\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m Boueny\u001b[38;5;241m.\u001b[39msave_database(database, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatabase.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\CogWorks\\BWSI_Language_Capstone\\Boueny.py:45\u001b[0m, in \u001b[0;36mDatabase.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerating caption embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaption_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_caption_embeddings(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcaption_data\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_embeddings\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Database' object has no attribute 'get_image_embeddings'"
     ]
    }
   ],
   "source": [
    "database = Database()\n",
    "Boueny.save_database(database, \"database.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30169267",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = Boueny.load_database(\"database.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder(512, 200)\n",
    "optim = SGD(model.parameters, learning_rate=1e-3)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "for epoch_cnt in range(180):\n",
    "    \n",
    "    triplets = database.get_training_batch(batch_size)\n",
    "    print(triplets[0][1].shape)\n",
    "    print(triplets[0][2].shape)\n",
    "    print(triplets[0][0].shape)\n",
    "    \n",
    "    triplets = [(caption_id, model(true_id), model(confusor_id)) for caption_id, true_id, confusor_id in triplets]\n",
    "    \n",
    "    for batch_cnt in range(0, len(dvs) // batch_size):\n",
    "        triplet = triplets[batchcnt]\n",
    "        \n",
    "        loss = loss_function_accuracy(triplet[1], triplet[0], triplet[2], 0.25)[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "with no_autodiff:\n",
    "    img_embeddings = model.encode(dvs).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92337c72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
